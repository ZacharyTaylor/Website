---
- !ruby/object:Paper
  title: Motion-Based Calibration of Multimodal Sensor Arrays
  conference: ICRA 2015
  abstract: This paper formulates a new pipeline for automated extrinsic calibration of multi-sensor mobile platforms. The new method can operate on any combination of cameras, navigation sensors and 3D lidars. Current methods for extrinsic calibration are either based on special markers and/or chequerboards, or they require a precise parameters initialisation for the calibration to converge. These two limitations prevent them from being fully automatic. The method presented in this paper removes these restrictions. By combining information extracted from both, platform’s motion estimates and external observations, our approach eliminates the need for special markers and also removes the need for manual initialisation. A third advantage is that the motion-based automatic initialisation does not require overlapping field of view between sensors. The paper also provides a method to estimate the accuracy of the resulting calibration. We illustrate the generalisation of our approach and validate its performance by showing results with two contrasting datasets. The first dataset was collected in a city with a car platform, and the second one was collected in a tree-crop farm with a Segway platform.
  file: ICRA2015.pdf
  image: ICRA2015.jpg
- !ruby/object:Paper
  title: Parameterless Automatic Extrinsic Calibration of Vehicle Mounted Lidar-Camera Systems
  conference: ICRA 2014 LTA Workshop
  abstract: This paper presents a new method for automated extrinsic calibration of multi-modal sensors. In particular the paper presents and evaluates a pipeline for calibration of 3D lidar and cameras mounted on a sensor vehicle. Previous methods for multi-modal sensor calibration find the optimal parameters by aligning a set of observations from the different sensor modalities. The main drawback of these methods is the need for a good initialisation in order to avoid converging into a local minima. Our approach eliminates this limitation by combining external observations with motion estimates obtained with the individual sensors. The method operates by utilizing structure from motion based hand-eye calibration to constrain the search space of the optimisation.
  file: ICRA2014_LTA.pdf
  image: ICRA2014_LTA.jpg
- !ruby/object:Paper
  title: Gradient Based Multi-modal Sensor Calibration
  conference: ICRA 2014 MEPC Workshop
  abstract: This paper presents an evaluation of a new metric for registering two sensors of different modality. The metric operates by aligning gradients present in the two sensors’ outputs. This metric is used to find the parameters between the sensors that minimizes the misalignment of the gradients. The metric can be applied to a wide range of problems and has been successfully demonstrated on the extrinsic calibration of two different lidar-camera systems as well as the alignment of IR and RGB images. Unlike most of previous techniques, our method requires no markers to be placed in the scene and can operate on a single scan from each sensor.
  file: ICRA2014_MEPC.pdf
  image: ICRA2014_MEPC.jpg
- !ruby/object:Paper
  title: Mapping clay minerals in an open-pit mine using hyperspectral imagery and automated feature extraction
  conference: VGC 2014
  abstract: The ability to map clay minerals on vertical geological surfaces is important from perspectives of stratigraphical mapping of geological units and safety. Clay minerals represent lines of stratigraphical weakness along which landslides can occur. To map clay minerals on complex geological surfaces we use a combination of hyperspectral and LiDAR data. These data are automatically registered to provide a map of the distribution of clay minerals and their abundances at different spatial scales.
  file: VGC2014.pdf
  image: VGC2014.jpg
- !ruby/object:Paper
  title: Multi-modal sensor calibration using a gradient orientation measure
  conference: JFR
  abstract: This paper presents a new metric for automated registration of multi-modal sensor data. The metric is based on the alignment of the orientation of gradients formed from the two candidate sensors. Data registration is performed by estimating the sensors’ extrinsic parameters that minimises the misalignment of the gradients. The metric can operate in a large range of applications working on both 2D and 3D sensor outputs and is suitable for both (i) single scan data registration and (ii) multi-sensor platform calibration using multiple scans. Unlike traditional calibration methods, it does not require markers or other registration aids to be placed in the scene. The effectiveness of the new method is demonstrated with experimental results on a variety of camera-lidar and camera-camera calibration problems. The novel metric is validated through comparisons with state of the art methods. Our approach is shown to give high quality registrations under all tested conditions.
  file: JFR2013.pdf
  image: JFR2013.jpg
- !ruby/object:Paper
  title: Automatic Calibration of Multi-Modal Sensor Systems using a Gradient Orientation Measure
  conference: IROS 2013
  abstract: A novel technique for calibrating a multi-modal sensor system has been developed. Our calibration method is based on the comparative alignment of output gradients from two candidate sensors. The algorithm is applied to the calibration of the extrinsic parameters of several camera-lidar systems. In this calibration the lidar scan is projected onto the camera's image using a camera model. Particle swarm optimization is used to find the optimal parameters for this model. This method requires no markers to be placed in the scene. While the system can use a set of scans, unlike many existing techniques it can also automatically calibrate the system reliably using a single scan. The method presented is successfully validated on a variety of cameras, lidars and locations. It is also compared to three existing techniques and shown to give comparable or superior results on the datasets tested.
  file: IROS2013.pdf
  image: IROS2013.jpg
- !ruby/object:Paper
  title: Orchard Fruit Segmentation using Multi-spectral Feature Learning
  conference: IROS 2013
  abstract: This paper presents a multi-class image segmentation approach to automate fruit segmentation. A feature learning algorithm combined with a conditional random field is applied to multi-spectral image data. Current classification methods used in agriculture scenarios tend to use hand crafted application-based features. In contrast, our approach uses unsupervised feature learning to automatically capture most relevant features from the data. This property makes our approach robust against variance in canopy trees and therefore has the potential to be applied to different domains. The proposed algorithm is applied to a fruit segmentation problem for a robotic agricultural surveillance mission, aiming to provide yield estimation with high accuracy and robustness against fruit variance. Experimental results with data collected in an almond farm are shown. The segmentation is performed with features extracted from multi-spectral (colour and infrared) data. We achieve a global classification accuracy of 88%.
  file: IROS2013_2.pdf
  image: IROS2013_2.jpg
- !ruby/object:Paper
  title: A Mutual Information Approach to Automatic Calibration of Camera and Lidar in Natural Environments
  conference: ACRA 2012
  abstract: This paper presents a method for calibrating the extrinsic and intrinsic parameters of a camera and a lidar scanner. The approach uses normalised mutual information to compare an image with a lidar scan. A camera model that takes into account orientation, location and focal length is used to create a 2D lidar image, with the intensity of the pixels defined by the angle of the normals in the lidar scan. Particle swarm optimisation is used to find the optimal model parameters. The method is successfully validated in a natural environment with images collected by a hyperspectral and a 3D lidar scanner.
  file: ACRA2012.pdf
  image: ACRA2012.jpg
- !ruby/object:Paper
  title: Automatic Calibration of Lidar with Camera Images using Normalized Mutual Information
  conference: Internal 2012
  abstract: This paper is about automatic calibration of a camera-lidar system. The method presented is designed to be as general as possible allowing it to be used in a large range of systems and applications. The approach uses normalized mutual information to compare the cameras image with an image generated by a lidar scan of the same area. A camera model that takes into account orientation, location and focal length is used to create the lidar image, with the intensity of the pixels in the image representing a feature of the lidar scan that is chosen depending on the application. Particle swarm optimization is used to find the optimal model parameters. The method presented is successfully validated on a variety of cameras, lidars and locations, including scans of both urban and natural environments.
  file: Internal2012.pdf
  image: Internal2012.jpg